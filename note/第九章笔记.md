## 9.1 了解GAN及其应用

GAN是一种框架，它将对抗性的过程用于生成模型的估计，在该过程中同时训练两个模型：生成器和鉴别器

生成模型（生成器）的目标是捕获训练集中包含的数据分布，而鉴别模型（鉴别器）充当二分类器，其目的是估计样本来自训练数据（而不是生成器）的概率。

其思想是在不明确定义损失函数的情况下训练生成模型，相反，我们使用来自另一个网络的信号作为反馈。生成器的目的是欺骗鉴别器，而鉴别器的目的是正确区分输入样本的真伪。

鉴别器D(x)是一个函数，它输出一个单标量，表示x来自实际数据分布而不是来自G(z)的概率

### 9.1.1 价值函数

价值函数是一种以预期收益表示博弈方目标的数学方法，GAN博弈的价值函数：

$\mathop{min}_\limits{G}\mathop{max}_\limits{D}V_{GAN}(D,G)=E_{x-P_{data}(x)}[logD(x)]+E_{z-p_z(z)}[log(1-D(G(z)))]$

鉴别器的目标是正确区分真实样本和伪造样本，该目标表示为$E_{x-P_{data}(x)}[logD(x)]$和$E_{z-p_z(z)}[log(1-D(G(z)))]$两项的最大化，前者表示对来自真实数据分布的样本的正确分类，而后者表示伪造样本的正确分类。

生成器被训练以欺骗鉴别器，其目标是最小化$E_{z-p_z(z)}[log(1-D(G(z)))]$，最小化这一项的方式是通过产生与真实样本越来越相似的样本，从而试图欺骗鉴别器。

### 9.1.2 非饱和价值函数

提出的解决方案是训练G以使$log(1-D(G(x)))$最大化而不是最小化。解决方案视为一种不同方式进行最小-最大博弈的方法。

鉴别器的目标是在不改变之前数学模型的前提下，最大限度地提高正确分类真假样本的概率。另外，生成器的目标是使得鉴别器将生成样本正确分类为假的概率最小化，但通过使鉴别器将伪造样本分类为真实样本本来显示地欺骗鉴别器。

博弈双方以不同的方式进行同一博弈的价值函数可以表示为

$$V_{GAN}(D,G)=\begin{cases}D:max_{D}E_{x-P_{data}(x)}[logD(x)]+E_{z-p_z(z)}[log(1-D(G(z)))] \\ G:max_{G}E_{z-P_{data}(z)}[logD(G(z))]\end{cases}$$

### 9.1.3 模型定义和训练阶段

将生成器和鉴别器定义为神经网络，是的我们可以使用多年来已开发的所有神经网络架构来解决这个问题，每个神经网络架构都专门用于处理某种特定的数据类型。

* 图片：卷积神经网络
* 序列、文本：递归神经网络
* 数值、类别值：全连接神经网络

博弈遵循以下规则：

**鉴别器**：鉴别器先执行，可以从1到k重复以下三个步骤，其中k是hyperparameter（通常k=1

1. 从$p_z(z)$之前的噪声中采样m个噪声样本：$z^{(1)},z^{(2)},...z^{(m)}$

2. 从实际数据分布$p_{data}(x)$中采样m个样本：$x^{(1)},...x^{(m)}$

3. 通过随机梯度上升法训练鉴别器：

   ​				$J=\frac{1}{m}\sum_\limits{i=1}^{m}logD(x^{(i)})+log(1-D(G)(z^{(i)}))$

   ​				$\theta_D+=\lambda\bigtriangledown_{\theta_D}J$

**生成器**：生成器始终在鉴别器一轮训练后训练，并且仅训练一次

1. 从$p_z(z)$之前的噪声中采样m个噪声样本：$z^{(1)},z^{(2)},...z^{(m)}$

2. 通过随机梯度上升法训练生成器（这是一个最大化问题，因为博弈的目标是非饱和价值函数：

   ​			$J=\frac{1}{m}\sum_\limits{i=1}^{m}log(D(G(z^{(i)})))$

   ​			$\theta_g+=\lambda\bigtriangledown_gJ$

## 9.2 无条件的GAN

GAN主要分为有条件的GAN和无条件的GAN

### 9.2.2 定义生成器

生成器的目标是像目标分布那样运行，因此，我们必须将其定义为具有单个神经元的网络。我们可以从目标分布中一次采样一个数字，从生成器中也可以这样做。

```python
def generator(input_shape):
    """
    :input_shape:the desired input shape (e.g.:(latent_space_size))
    :return: the generator model
    """
    inputs = tf.keras.layers.Input(input_shape)
    net = tf.keras.layers.Dense(64,activation=tf.nn.relu,name='fc1')(inputs)
    net = tf.keras.layers.Dense(64,activation=tf.nn.relu,name='fc2')(net)
    net = tf.keras.layers.Dense(1,name='G')(net)
    G = tf.keras.Model(inputs=inputs,outputs=net)
    return G
```

### 9.2.3 定义鉴别器

```python
def discriminator(input_shape):
    """
    defines the discriminator keras.Model
    :param input_shape:the desired input shape(e.g.:(the generator output shape)) 
    
    :return: D: the discriminator model
    """
    inputs = tf.keras.layers.Input(input_shape)
    net = tf.keras.layers.Dense(32,activation=tf.nn.relu,name='fc1')(inputs)
    net = tf.keras.layers.Dense(1,name="D")(net)
    D = tf.keras.Model(inputs=inputs,outputs=net)
    return D
```

### 9.2.4 定义损失函数

为了遵循原始模型来实现对抗训练的过程，要使用的loss函数是二进制交叉熵

```python
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
```

bce对象计算两个分布之间的二进制交叉熵：

* 使用鉴别器的输出来表示，其值被压缩到[0,1]范围内（通过对其应用sigmoid函数，因为from_logits参数设为True）,如果鉴别器将分类为来自真实数据分布，则生成的值将接近于1
* 类别标签上的条件经验分布，即离散概率分布，其中它是真实样本的概率记为1，否则为0

数学上，类别标签(y)上的条件经验分布与压缩在[0,1]范围内生成器输出$\hat y=\sigma(D(x))$之间的二进制交叉熵表示如下：

​				$L_{BCE}=ylog(\hat y)-(1-y)log(1-\hat y)$

我们要训练鉴别器正确分类真实数据和伪造数据：正确分类真实数据可以看作是$E_{x-P_{data}(x)}[logD(x)]$最大化，而正确分类$E_{z-p_z(z)}[log(1-D(G(z)))]$的最大化。

将一批m个样本的期望值使用经验平均值替换，可以将正确分类样本的对数概率的最大值表示为两个BCE的总和：

$\frac{1}{m}\sum_\limits{i=1}^m-log\sigma(D(x))+\frac{1}{m}\sum_\limits{i=1}^{m}-log(1-\sigma(D(G(z))))$

第一项是给定真实样本作为输入时，标签y=1与鉴别器输出之间BCE，而第二项是给定伪造样本作为输入时，标签y=0与鉴别器输出之间的BCE。

```python
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
def d_loss(d_real,d_fake):
    """The discriminator loss function."""
    return bce(tf.ones_like(d_real),d_real) + bce(tf.zeros_like(d_fake),d_fake)
```

**生成器损失函数**：$-\frac{1}{m}\sum_\limits{i=1}^{m}\sigma(log(D(G'(z))))$

该公式是生成图像的对数概率和真实图像（标记为1）的分布之间的二进制交叉熵

```python
def g_loss(generated_output):
    """The Generator loss function"""
    return bce(tf.ones_like(generated_output),generated_output)
```

### 9.2.5 无条件的GAN中的对抗训练过程

对抗训练的过程是我们交替执行鉴别器和生成器训练步骤的过程。生成器需要由鉴别器计算出的值来执行其参数的更新，而鉴别器需要生成样本（也就称为伪输入）和真实样本。此训练过程可能需要大量的计算，值得使用@tf.function修饰训练步骤函数，以便通过把它转换成一个图来加快计算

```python
def train():
    # Define the optimizers and the train operations
    optimizer = tf.keras.optimizers.Adam(1e-5)
    @tf.function
    def train_step():
        with tf.GradientTape(persistent=True) as tape:
            real_data = sample_dataset()
            noisy_vector = tf.random.normal(mean=0,stddev=1,shape=(real_data.shape[0],latent_space_shape[0]))
            # Sample from the Generator
            fake_data = G(noisy_vector)
            # Compute the D loss
            d_fake_data = D(fake_data)
            d_real_data = D(real_data)
            d_loss_value = d_loss(d_real_data,d_fake_data)
            # Compute the G loss
            g_loss_value = g_loss(d_fake_data)

        # Now that we computed the losses we can compute the gradient and optimize the networks
        d_gradients = tape.gradient(d_loss_value,D.trainable_variables)
        g_gradients = tape.gradient(g_loss_value,G.trainable_variables)
        # Deleting the tape, since we define it as persistent
        # (because we used it twice
        del tape
        optimizer.apply_gradients(zip(d_gradients,D.trainable_variables))
        optimizer.apply_gradients(zip(g_gradients,G.trainable_variables))
        return real_data,fake_data,g_loss_value,d_loss_value
    
```

train_step函数是整个代码段中最重要的功能，通过使用trainable_variables，可以在考虑其他所有常数的同时，计算损失函数对于我们感兴趣的模型参数的梯度。

第二个特性是使用持久的梯度磁带对象。当内存中分配单个对象并使用两次时，使用持久性tape可以使我们跟踪执行情况。如果tape不是持久性创建的，我们将无法重用他们，因为在第一次.gradient调用后它将自动销毁。

## 9.3 有条件的GAN

条件GAN的基本思想：如果G和D都以某种附加信息y为条件，则GAN可以扩展为条件模型。此附加信息可以为任何类型的附加信息，从类标签到语义映射，或其他形式的数据。通过将附加信息作为附加输入层输入到生成器和鉴别器中，可以执行这种条件化的调节。

在此基础上进一步扩展了生成器的结构，以将条件之前的噪声的联合隐藏表示结合起来。对于如何将条件反馈给生成器网络没有任何限制。可以简单地将条件连接到噪声向量，或者，如果条件复杂，则可以使用NN对其进行编码，并将其输出连接到生成器的一层。同样也适用于鉴别器。

$\mathop{min}_\limits{G}\mathop{max}_\limits{D}V_{GAN}(D,G)=E_{x\sim P_{data}(x|y)}[logD(x,y)]+E_{z-p_z(z)}[log(1-D(G(z|y),y))]$

### 9.3.1 为有条件的GAN获取数据

```python
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

dataset = tfds.load('fashion_mnist',split='train')
def convert(row):
    image = tf.image.convert_image_dtype(row['image'],tf.float32)
    label = tf.cast(row['label'],tf.float32)
    return image,label
batch_size = 32
dataset = dataset.map(convert).batch(batch_size).prefetch(1)
```

### 9.3.2 在有条件的GAN中定义生成器

```python

def get_generator(latent_dimension):
    # Condition subnetwork: encode the condition in a hidden representation
    condition = tf.keras.layers.Input((1,))
    net = tf.keras.layers.Dense(32,activation=tf.nn.elu)(condition)
    net = tf.keras.layers.Dense(64,activation=tf.nn.elu)(net)
    # Concatenate the hidden condition representation to noise and upsample
    noise = tf.keras.layers.Input(latent_dimension)
    inputs = tf.keras.layers.Concatenate()([noise,net])
    # Convert inputs from (batch_size, latent_dimension + 1)
    # To a 4-D tensor,that can be used
    inputs = tf.keras.layers.Reshape((1,1,inputs.shape[-1]))(inputs)
    depth = 128
    kernel_size = 5
    net = tf.keras.layers.Conv2DTranspose(depth,kernel_size,padding='valid',strides=1,activation=tf.nn.relu)(inputs)
    net = tf.keras.layers.Conv2DTranspose(depth//2,kernel_size,padding='valid',strides=2,activation=tf.nn.relu)(net)
    net = tf.keras.layers.Conv2DTranspose(depth//4,kernel_size,strides=2,padding='valid',
                                          activation=tf.nn.relu,use_bias=False)(net)

    # Standard convolutional with 2*2 kernel to obtain a 28*28*1 out
    net = tf.keras.layers.Conv2D(1,2,padding='valid',strides=1,activation=tf.nn.sigmoid,use_bias=False)(net)
    model = tf.keras.Model(inputs=[noise,condition],outputs=net)
    

```

### 9.3.3 在有条件的GAN中定义鉴别器

条件话调节鉴别器的一种标准方法是将图像的编码与条件的编码共同存储在同一个向量中

为此需要定义两个子网络——第一个子网络将图像编码到特征向量中，而第二个子网络将条件编码到另一个向量中。

```python

def get_Discriminator():
    # Encoder subnetwork: feature extractor to get a feature vector
    image = tf.keras.layers.Input((28,28,1))
    depth = 32
    kernel_size = 3
    net = tf.keras.layers.Conv2D(depth,kernel_size,padding='same',strides=2,activation=tf.nn.relu)(image)
    net = tf.keras.layers.Conv2D(depth*2,kernel_size,padding='same',strides=2,activation=tf.nn.relu)(net)
    net = tf.keras.layers.Conv2D(depth*3,kernel_size,padding='same',strides=2,activation=tf.nn.relu)(net)
    feature_vector = tf.keras.layers.Flatten()(net)


    # Create a hidden representation of the condition
    condition = tf.keras.layers.Input((1,))
    hidden = tf.keras.layers.Dense(32,activation=tf.nn.elu)(condition)
    hidden = tf.keras.layers.Dense(64,activation=tf.nn.elu)(hidden)
    # Concatenate the feature vector and the hidden label representation
    out = tf.keras.layers.Concatenate()([feature_vector,hidden])
    # Add the final classification layers with a single linear neuron
    out = tf.keras.layers.Dense(128,activation=tf.nn.relu)(out)
    out = tf.keras.layers.Dense(1)(out)
    model = tf.keras.Model(inputs=[image,condition],outputs=out)
    return model
```

### 9.3.4 对抗训练过程

```python
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
def d_loss(d_real,d_fake):
    """the discriminator loss function"""
    return bce(tf.ones_like(d_real),d_real) + bce(tf.zeros_like(d_fake))
def g_loss(generated_output):
    """The Generator loss function."""
    return bce(tf.ones_like(generated_output),generated_output)
latent_dimension = 100
G = get_generator(latent_dimension)
D = get_Discriminator()

def train():
    # Define the optimizers and the train operations
    optimizer = tf.keras.optimizers.Adam(1e-5)
    @tf.function
    def train_step(image, label):
        with tf.GradientTape(persistent=True) as tape:
            noise_vector = tf.random.normal(
                mean=0,stddev=1,shape=(image.shape[0],latent_dimension)
            )
            # Sample from the Generator
            fake_data = G([noise_vector,label])
            # Compute the D loss
            d_fake_data = D([fake_data,label])
            d_real_data = D([image,label])
            d_loss_value = d_loss(d_real_data,d_fake_data)
            # Compute the G loss
            g_loss_value = g_loss(d_fake_data)
        #Now that we computed the loss we can compute the gradient and optimize the networks
        d_gradients = tape.gradient(d_loss_value,D.trainable_variables)
        g_gradients = tape.gradient(g_loss_value,G.trainable_variables)

        # deleting the tape, since we defined it as persistent
        del tape
        optimizer.apply_gradients(zip(d_gradients,D.trainable_variables))
        optimizer.apply_gradients(zip(g_gradients,G.trainable_variables))
        return g_loss_value,d_loss_value,fake_data[0],label[0]
    epochs = 10
    for epoch in range(epochs):
        for image,label in dataset:
            g_loss_value, d_loss_value, generated, condition = train_step(image,label)

            print('epoch ',epoch,"complete")
            print('loss: ',g_loss_value,'d_loss: ',d_loss_value)
            plt.imshow(tf.squeeze(generated).numpy(),cmap=='gray')
            plt.show()
```